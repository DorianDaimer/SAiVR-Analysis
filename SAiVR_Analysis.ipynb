{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAiVR Analysis Notebook**\n",
    "\n",
    "This notebook contains code for analysing and visualizing the results of the SAiVR study project.\n",
    "Currently the focus lies in analysing the results of the three different tasks: Absolute orientation task, Relative orientation task and Pointing task.\n",
    "To work with this notebook you need to have the google spreadsheet of the NBP-VR-Lab and the mat files containing the results of the subjects stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io as spio\n",
    "from scipy.spatial import distance\n",
    "#import ezodf\n",
    "from matplotlib.patches import Arrow, Circle\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import ptitprince as pt\n",
    "#from __future__ import print_function\n",
    "from statsmodels.compat import lzip\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the correct path information for the google spreadsheet and the mat files to read them in. This has to be adapted to the individual location on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path leading to the folder with the mat files\n",
    "taskPath = \"C:/Users/mein/Desktop/Cognitive Science/Saivr/Data/TaskResults\"\n",
    "# define the path to the xlsx file containing the spreadsheet of the lab\n",
    "calenderPath = \"C:/Users/mein/Desktop/Cognitive Science/Saivr/Data/Seahaven_alingment_project.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import calender file, only load specified columns\n",
    "df = pd.read_excel(calenderPath, usecols='A,C:G')\n",
    "df[\"Training\"] = df[\"Training\"].str.lower()\n",
    "df = df[df.Discarded != 'yes']\n",
    "# only keep identifier (letter)\n",
    "df[\"Repeated\"]= df[\"Repeated\"].str[1:]\n",
    "# drop first faulty measurement of f for experimental subjects and c guy who was motion sick\n",
    "df = df[df[\"Subject#\"]!=4616]\n",
    "df = df[df[\"Subject#\"]!=1338]\n",
    "# drop faulty control measurements\n",
    "df = df[df[\"Subject#\"]!=8269]\n",
    "df = df[df[\"Subject#\"]!=8815]\n",
    "# identifiers of successful control particpants\n",
    "ids = [\"a\", \"b\", \"c\", \"i\", \"p\", \"k\", \"q\"]\n",
    "c_measure = df[df.Training == \"belt_c\"][[\"Subject#\", \"Measurement#\", \"Repeated\", \"Training\"]]\n",
    "# only store whether the participant was in the experimental or the control group ('e' or 'c')\n",
    "c_measure[\"Training\"] = c_measure[\"Training\"].str[-1]\n",
    "# only take successful control participants\n",
    "c_measure = c_measure[c_measure['Repeated'].isin(ids)]\n",
    "# get the data on all experimental participants\n",
    "exp_measure = df[df.Training == \"belt_e\"][[\"Subject#\", \"Measurement#\", \"Repeated\", \"Training\"]]\n",
    "exp_measure[\"Training\"] = exp_measure[\"Training\"].str[-1]\n",
    "# combine control and experimental subjects in one data frame\n",
    "measure_df = pd.concat([c_measure, exp_measure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_to_py(AlignmentPath,number):\n",
    "    '''\n",
    "    converts mat struct with task results into (numpy) array\n",
    "\n",
    "    also adds extra column with information whether trial was correct or wrong\n",
    "    \n",
    "    conditions = [\"Absolute - 3s \",\"Absolute - inf\",\"Relative - 3s \",\"Relative - inf\",\"Pointing 3s   \",\"Pointing - inf\"]\n",
    "    '''\n",
    "    path = AlignmentPath+\"/AlignmentVR_SubjNo_\"+number+\".mat\"\n",
    "    mat_contents = spio.loadmat(path)\n",
    "    type_array = []\n",
    "    for i,cond_1 in enumerate([\"Absolute\", \"Relative\",\"Pointing\"]):\n",
    "        for j,cond_2 in enumerate([\"Trial_3s\", \"Trial_Inf\"]):\n",
    "            trials_array = []\n",
    "            for line in range(len(mat_contents['Output'][0][0][cond_1][cond_2][0][0])):\n",
    "                value_array = []\n",
    "                for column in range(len(mat_contents['Output'][0][0][cond_1][cond_2][0][0][line][0])):\n",
    "                    value = mat_contents['Output'][0][0][cond_1][cond_2][0][0][line][0][column][0][0]\n",
    "                    value_array.append(value)\n",
    "                # check if trial is correct(true or false\n",
    "                value_array.append(value_array[-1] == value_array[-3])\n",
    "                trials_array.append(value_array)\n",
    "\n",
    "            type_array.append(trials_array)\n",
    "\n",
    "    return np.array(type_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mat_to_py function and measures_df data frame to load the performance of all subjects on all tasks into the AllResults data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\"Absolute - 3s \",\"Absolute - inf\",\"Relative - 3s \",\"Relative - inf\",\"Pointing 3s   \",\"Pointing - inf\"]\n",
    "# get all vp_nums from measure_df data frame\n",
    "vp_nums = measure_df[\"Subject#\"].astype(str).tolist()\n",
    "AllResults = np.zeros((6,len(vp_nums),36))#AllResults[condition][subjectNum][Trial]\n",
    "for i,e in enumerate(vp_nums):\n",
    "    try:\n",
    "        m = mat_to_py(taskPath,e)\n",
    "        for c in range(6):       \n",
    "            condperf = []\n",
    "            for t in range(36):\n",
    "                condperf.append(int(m[c][t][-1]))\n",
    "            AllResults[c][i] = condperf  \n",
    "    except:\n",
    "        print(str(e)+\" Not in folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a performance data frame containing the perfromance (percentage of correct answers) of \n",
    "# each subject on each task and join this with vpN to get vp_numbers as index\n",
    "performances = pd.DataFrame()\n",
    "vpN = pd.DataFrame(vp_nums,columns=['vp_number']).astype(int)\n",
    "for cond in range(6):\n",
    "    performances[cond] = np.mean(AllResults[cond],axis=1)\n",
    "performances.columns = conditions\n",
    "performances = vpN.join(performances).set_index('vp_number')\n",
    "#performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some overall stats on the performance df\n",
    "performances.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use performance df and measure_df to create a new data frame containing all the information relevant for further analysis.\n",
    "The final data frame to work with is AllPerformances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge performances and measure_df on vp_numbers\n",
    "df_all = performances.merge(measure_df, left_on=\"vp_number\", right_on=\"Subject#\").set_index(\"Subject#\")\n",
    "#df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct AllPerformances frame\n",
    "TaskList = ['Absolute','Absolute','Relative','Relative','Pointing','Pointing']\n",
    "CondList = ['3s','inf','3s','inf','3s','inf']\n",
    "AllPerformances = pd.DataFrame(columns=['Subject','Task','Condition','Performance', 'Measurement', 'Training'])\n",
    "for sj in list(df_all.index):\n",
    "    for i,c in enumerate(conditions):\n",
    "        AllPerformances = AllPerformances.append({'Subject':df_all.loc[sj][\"Repeated\"],'Task':TaskList[i],'Condition':CondList[i],'Performance':df_all.loc[sj][c],'Measurement':df_all.loc[sj][\"Measurement#\"],'Training':df_all.loc[sj]['Training']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"TC\" column as combination of Task and Condition for later plotting\n",
    "AllPerformances[\"TC\"] = AllPerformances.Task + \" - \" + AllPerformances.Condition\n",
    "# store data of experimental and control group in seperate frames to make plotting easier\n",
    "AllPerformances_e = AllPerformances[AllPerformances[\"Training\"] == 'e']\n",
    "AllPerformances_c = AllPerformances[AllPerformances[\"Training\"] == 'c']\n",
    "AllPerformances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AllPerformances contains 7 columns. Subject column identifies each participant with a unique letter. These letters are only unique for when control and experimental group are considered independently. The combination of Training and Subject columns is a unique identifier for each individual subject however. Task contains the name of the task. Condition stores information on the time condition. Performance stores the actual performance for the task and time condition. The measurement column indicates which measurement of the given subject we are considering (1-4). Training stoes information on whether the subject was in the experimental or the control group ('e' or 'c') and TC contains the combination of task and condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the data**\n",
    "\n",
    "First we have a look at boxplots displaying the performance results for the three different tasks and the two different time conditions. This is done seperately for experimental and control subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group tasks\n",
    "#color by time condition\n",
    "fig,ax = plt.subplots(figsize=(10,7))\n",
    "# add the chance level line\n",
    "plt.plot([-5,10],[0.5,0.5],':',color='black', linewidth=5)\n",
    "# create the boxplots\n",
    "sns.boxplot(data=AllPerformances_e,hue='Condition',x='Task',y='Performance', palette=[\"red\", \"royalblue\"],linewidth=2.5)\n",
    "# set the aestethics\n",
    "ax.set_xticklabels(['Absolute','Relative','Pointing'],fontsize=15)\n",
    "ax.set_ylim((0,1))\n",
    "plt.legend(fontsize=20,loc=4)\n",
    "plt.title('Performance of exp. subjects in the tasks',fontsize=25)\n",
    "plt.ylabel('Performance (%)',fontsize=20)\n",
    "plt.yticks(np.linspace(0,1,5),np.linspace(0,100,5,dtype=int),fontsize=15)\n",
    "plt.xlabel(\"Task\",fontsize=20);\n",
    "#plt.show()\n",
    "#plt.savefig('Results/TaskPerformancesGrouped.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group tasks\n",
    "#color by time condition\n",
    "fig,ax = plt.subplots(figsize=(10,7))\n",
    "plt.plot([-5,10],[0.5,0.5],':',color='black', linewidth=5)\n",
    "sns.boxplot(data=AllPerformances_c,hue='Condition',x='Task',y='Performance', palette=[\"red\", \"royalblue\"],linewidth=2.5)\n",
    "ax.set_xticklabels(['Absolute','Relative','Pointing'],fontsize=15)\n",
    "ax.set_ylim((0,1))\n",
    "plt.legend(fontsize=20,loc=4)\n",
    "plt.title('Performance of cont. subjects in the tasks',fontsize=25)\n",
    "plt.ylabel('Performance (%)',fontsize=20)\n",
    "plt.yticks(np.linspace(0,1,5),np.linspace(0,100,5,dtype=int),fontsize=15)\n",
    "plt.xlabel(\"Task\",fontsize=20);\n",
    "#plt.show()\n",
    "#plt.savefig('Results/TaskPerformancesGrouped.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a look at the performance for the different measurements. Again this is done seperately for each combination of task and time condition and for experimental and control subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='TC', y='Performance', hue='Measurement', data=AllPerformances_e, kind='bar', height=7, aspect=2)\n",
    "#plt.plot([-0.45,5.4],[0.5,0.5],':',color='black', linewidth=3)\n",
    "#plt.legend(fontsize=20,loc=4)\n",
    "plt.title('Performance of exp. subjects in the tasks for different measurements',fontsize=25)\n",
    "plt.ylabel('Performance (%)',fontsize=20)\n",
    "plt.yticks(np.linspace(0,0.7,8),np.linspace(0,70,8,dtype=int),fontsize=15)\n",
    "plt.xlabel(\"Task and Condition\",fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='TC', y='Performance', hue='Measurement', data=AllPerformances_c, kind='bar', height=7, aspect=2)\n",
    "#plt.plot([-0.45,5.4],[0.5,0.5],':',color='black', linewidth=3)\n",
    "#plt.legend(fontsize=20,loc=4)\n",
    "plt.title('Performance of cont. subjects in the tasks for different measurements',fontsize=25)\n",
    "plt.ylabel('Performance (%)',fontsize=20)\n",
    "plt.yticks(np.linspace(0,0.7,8),np.linspace(0,70,8,dtype=int),fontsize=15)\n",
    "plt.xlabel(\"Task and Condition\",fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have a look at a raincloud plot. Here we mix experimental and control subjects to visualize all the available data once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting adapted from https://peerj.com/preprints/27137v1/\n",
    "ax = pt.RainCloud(data=AllPerformances,hue='Condition',x='Task',y='Performance', palette=[\"red\", \"royalblue\"],bw = 0.2,\n",
    "                 width_viol = .5, figsize = (10,7),pointplot = False, alpha = .85, dodge = True, move = 0.2)\n",
    "\n",
    "ax.set_xticklabels(['Absolute','Relative','Pointing'],fontsize=15)\n",
    "#ax.legend(['3s','inf'],fontsize=20,loc=1)\n",
    "\n",
    "plt.title('Performance of Subjects in the Tasks',fontsize=25)\n",
    "plt.ylabel('Performance (%)',fontsize=20)\n",
    "plt.xlabel(\"Task\",fontsize=20)\n",
    "plt.yticks(np.linspace(0.25,0.75,3),np.linspace(25,75,3),fontsize=15);\n",
    "#plt.show()\n",
    "#plt.savefig('Results/TaskPerformancesRainCloud.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistics - Dive into the data**\n",
    "\n",
    "After having had a look at different visualizations of the data, we are going to do some more rigorous analysis. We start of investigating the performance in relation to different dependent variables, such as Measurement or Condition and Task. In case any of those variables reveals itself to have a significant influence on the performance, we perform paired t-tests to pinpoint the relevant value for which the mean performance differs from the rest.\n",
    "Again we treat control and experimental subjects seperately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at the experimental group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anova experimental group\n",
    "anovarm = AnovaRM(data=AllPerformances_e,depvar='Performance',subject='Subject',within=['Task','Condition','Measurement'])\n",
    "fit = anovarm.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anova exp, no k, j  -> include only participants that had all measurements completed at the time of writing (11.07.19)\n",
    "# keep this and the analysis done on test frame for now and out of curiosity, but ultimately we care about AllPerformances_e\n",
    "test = AllPerformances_e[AllPerformances_e['Subject']!='k']\n",
    "test = test[test['Subject']!='j']\n",
    "anovarm = AnovaRM(data=test,depvar='Performance',subject='Subject',within=['Task','Condition','Measurement'])\n",
    "fit = anovarm.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of significant variables here. We will proceed with paired t-tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performances for each task\n",
    "Abs = AllPerformances_e[AllPerformances_e['Task']=='Absolute']['Performance']\n",
    "Rel = AllPerformances_e[AllPerformances_e['Task']=='Relative']['Performance']\n",
    "Ptg = AllPerformances_e[AllPerformances_e['Task']=='Pointing']['Performance']\n",
    "# print the reults of the paired t-tests for all possible combinations\n",
    "print(\"Abs - Rel: \"+str(stats.ttest_rel(Abs, Rel)))\n",
    "print(\"Abs - Ptg: \"+str(stats.ttest_rel(Abs, Ptg)))\n",
    "print(\"Rel - Ptg: \"+str(stats.ttest_rel(Rel, Ptg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performances for each task\n",
    "Abs = test[test['Task']=='Absolute']['Performance']\n",
    "Rel = test[test['Task']=='Relative']['Performance']\n",
    "Ptg = test[test['Task']=='Pointing']['Performance']\n",
    "# print the reults of the paired t-tests for all possible combinations\n",
    "print(\"Abs - Rel: \"+str(stats.ttest_rel(Abs, Rel)))\n",
    "print(\"Abs - Ptg: \"+str(stats.ttest_rel(Abs, Ptg)))\n",
    "print(\"Rel - Ptg: \"+str(stats.ttest_rel(Rel, Ptg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performances for each condition\n",
    "Short = AllPerformances_e[AllPerformances_e['Condition']=='3s']['Performance']\n",
    "Inf = AllPerformances_e[AllPerformances_e['Condition']=='inf']['Performance']\n",
    "# print the reults of the paired t-tests for all possible combinations\n",
    "print(\"Short - Inf: \"+str(stats.ttest_rel(Short, Inf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performances for each measurement\n",
    "Measurement_1 = test[test['Measurement']==1]['Performance']\n",
    "Measurement_2 = test[test['Measurement']==2]['Performance']\n",
    "Measurement_3 = test[test['Measurement']==3]['Performance']\n",
    "Measurement_4 = test[test['Measurement']==4]['Performance']\n",
    "# print the reults of the paired t-tests for all possible combinations\n",
    "print(\"M_1 - M_2: \"+str(stats.ttest_rel(Measurement_1, Measurement_2)))\n",
    "print(\"M_1 - M_3: \"+str(stats.ttest_rel(Measurement_1, Measurement_3)))\n",
    "print(\"M_1 - M_4: \"+str(stats.ttest_rel(Measurement_1, Measurement_4)))\n",
    "print(\"M_2 - M_3: \"+str(stats.ttest_rel(Measurement_2, Measurement_3)))\n",
    "print(\"M_2 - M_4: \"+str(stats.ttest_rel(Measurement_2, Measurement_4)))\n",
    "print(\"M_3 - M_4: \"+str(stats.ttest_rel(Measurement_3, Measurement_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performances for each measurement\n",
    "Measurement_1 = AllPerformances_e[AllPerformances_e['Measurement']==1]['Performance']\n",
    "Measurement_2 = AllPerformances_e[AllPerformances_e['Measurement']==2]['Performance']\n",
    "Measurement_3 = AllPerformances_e[AllPerformances_e['Measurement']==3]['Performance']\n",
    "Measurement_4 = AllPerformances_e[AllPerformances_e['Measurement']==4]['Performance']\n",
    "# print the reults of the paired t-tests for all possible combinations\n",
    "print(\"M_1 - M_2: \"+str(stats.ttest_rel(Measurement_1, Measurement_2)))\n",
    "print(\"M_1 - M_3: \"+str(stats.ttest_rel(Measurement_1, Measurement_3)))\n",
    "print(\"M_1 - M_4: \"+str(stats.ttest_rel(Measurement_1, Measurement_4)))\n",
    "print(\"M_2 - M_3: \"+str(stats.ttest_rel(Measurement_2, Measurement_3)))\n",
    "print(\"M_2 - M_4: \"+str(stats.ttest_rel(Measurement_2, Measurement_4)))\n",
    "print(\"M_3 - M_4: \"+str(stats.ttest_rel(Measurement_3, Measurement_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performances for each measurement and short time condition\n",
    "M_1 = test[test['Measurement']==1]\n",
    "M_2 = test[test['Measurement']==2]\n",
    "M_3 = test[test['Measurement']==3]\n",
    "M_4 = test[test['Measurement']==4]\n",
    "# get short time condition\n",
    "MC_1s = M_1[M_1['Condition']=='3s']['Performance']\n",
    "MC_2s = M_2[M_2['Condition']=='3s']['Performance']\n",
    "MC_3s = M_3[M_3['Condition']=='3s']['Performance']\n",
    "MC_4s = M_4[M_4['Condition']=='3s']['Performance']\n",
    "# get long time condition\n",
    "MC_1l = M_1[M_1['Condition']=='inf']['Performance']\n",
    "MC_2l = M_2[M_2['Condition']=='inf']['Performance']\n",
    "MC_3l = M_3[M_3['Condition']=='inf']['Performance']\n",
    "MC_4l = M_4[M_4['Condition']=='inf']['Performance']\n",
    "# print the reults of the paired t-tests for all possible combinations\n",
    "print(\"MC_1s - MC_2s: \"+str(stats.ttest_rel(MC_1s, MC_2s)))\n",
    "print(\"MC_1s - MC_3s: \"+str(stats.ttest_rel(MC_1s, MC_3s)))\n",
    "print(\"MC_1s - MC_4s: \"+str(stats.ttest_rel(MC_1s, MC_4s)))\n",
    "print(\"MC_2s - MC_3s: \"+str(stats.ttest_rel(MC_2s, MC_3s)))\n",
    "print(\"MC_2s - MC_4s: \"+str(stats.ttest_rel(MC_2s, MC_4s)))\n",
    "print(\"MC_3s - MC_4s: \"+str(stats.ttest_rel(MC_3s, MC_4s)))\n",
    "\n",
    "print(\"MC_1l - MC_2l: \"+str(stats.ttest_rel(MC_1l, MC_2l)))\n",
    "print(\"MC_1l - MC_3l: \"+str(stats.ttest_rel(MC_1l, MC_3l)))\n",
    "print(\"MC_1l - MC_4l: \"+str(stats.ttest_rel(MC_1l, MC_4l)))\n",
    "print(\"MC_2l - MC_3l: \"+str(stats.ttest_rel(MC_2l, MC_3l)))\n",
    "print(\"MC_2l - MC_4l: \"+str(stats.ttest_rel(MC_2l, MC_4l)))\n",
    "print(\"MC_3l - MC_4l: \"+str(stats.ttest_rel(MC_3l, MC_4l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a look at the controls. There are only 7 fully measured control participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Anova control\n",
    "anovarm = AnovaRM(data=AllPerformances_c,depvar='Performance',subject='Subject',within=['Task','Condition','Measurement'])\n",
    "fit = anovarm.fit()\n",
    "fit.summary()\n",
    "# do t-tests for measurement and stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only the measurement is significant, we perform post-hoc paired t-tests to identify for which measurement the mean performance significantly differs from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performances for each measurement\n",
    "Measurement_1 = AllPerformances_c[AllPerformances_c['Measurement']==1]['Performance']\n",
    "Measurement_2 = AllPerformances_c[AllPerformances_c['Measurement']==2]['Performance']\n",
    "Measurement_3 = AllPerformances_c[AllPerformances_c['Measurement']==3]['Performance']\n",
    "Measurement_4 = AllPerformances_c[AllPerformances_c['Measurement']==4]['Performance']\n",
    "# print the reults of the paired t-tests for all possible combinations\n",
    "print(\"M_1 - M_2: \"+str(stats.ttest_rel(Measurement_1, Measurement_2)))\n",
    "print(\"M_1 - M_3: \"+str(stats.ttest_rel(Measurement_1, Measurement_3)))\n",
    "print(\"M_1 - M_4: \"+str(stats.ttest_rel(Measurement_1, Measurement_4)))\n",
    "print(\"M_2 - M_3: \"+str(stats.ttest_rel(Measurement_2, Measurement_3)))\n",
    "print(\"M_2 - M_4: \"+str(stats.ttest_rel(Measurement_2, Measurement_4)))\n",
    "print(\"M_3 - M_4: \"+str(stats.ttest_rel(Measurement_3, Measurement_4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (acc)",
   "language": "python",
   "name": "acc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
